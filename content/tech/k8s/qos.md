---
title: "k8s服务质量调优"
date: 2020-05-08T16:31:23+08:00
draft: true
tags:
- k8s
dropCap: true
indent: instead
---
### 关于服务质量
我的理解是，服务质量即提供高效、稳定的服务的能力，这应该作为每个服务的终极目标。在比较有限的硬件资源支持下通过调优使服务有更高的稳定性。就k8s可以通过对关键服务，比如数据库、服务注册发现服务、网关等进行更优先的资源分配，同时通过nodeSelector（节点选择）、亲和性（Affinity）/反亲和性（anti-affinity）、容忍（Taints）/污点（tolerations）等调度策略的设置将关键服务调度到资源更加充足的节点，其余非关键服务可以通过设置服务质量等级（QoS）让k8s进行最优调度。保证在资源紧张的时候，重要服务优先运行，避免关键服务宕机导致所有关联服务不可用，其他"非重要"服务可以等待资源的弹性伸缩完成后恢复服务。

从`docker swarm`迁移到`k8s`已经有一段时间，花了很大的时间和精力，期间有过三次磁盘挂载异常的灾难级故障，而后又有周期性的节点故障。由于日常紧急开发任务比较多，每次遇到故障都是直接移除节点再添加，或者在开发和测试环境直接是强制重启节点以恢复正常运行。起初猜测是没有进行服务质量等级调优导致个别节点被调度太多服务，导致节点宕机。后面一次深入查证果然是因为内存溢出导致的各种诡异问题的出现。现在有时间回头处理这个事情了。记录之。

### 服务现状以及调优过程
服务质量等级以及pod调度策略相关资料是很完善的，不再赘述。本文直接结合实际场景记录调优过程，旨在分享一个建议的调优思路和经验。

上`k8s`之后将开发环境和测试环境也直接上云，本地通过`telepresence`工具连接`k8s`以实现本地开发和调试。可以先根据服务重要程度进行排序再根据服务大致的资源占用指标确定调优方案。本文以我司开发环境和测试环境所在的集群为例。

##### `k8s`命名空间划分如下：

|命名空间|用途|
|---|---|
|cert-manager|部署证书管理相关服务|
|cicd-system|运行流水线、私有镜像仓库等运维相关服务|
|default|默认命名空间，主要运行`telepresence`的服务端`pod`|
|dev|运行开发环境完整服务、包括数据库、服务注册与发现等`关键服务`|
|istio-system|运行`istio`关键服务，包括网关|
|kube-public|默认命名空间，没有作用|
|kube-system|默认命名空间，运行`k8s`系统服务|
|testing|运行测试环境完整服务|
|harbor-online|运行harbor面向公网的服务（内网服务运行在cicd-system命名空间中）。|

##### 关键服务排序(除去 kube-system 从最重要往后排序)

|服务名称|所在命名空间|用途|
|---|---|---|
|postgresql|cicd-system|为私有仓库harbor、drone等服务提供存储支持|
|redis|cicd-system|为私有仓库harbor提供镜像层数据缓存支持|
|harbor|cicd-system|为整个公司提供私有镜像存储服务，本服务异常可能导致线上服务因不能拉取镜像而无法启动|
|drone|cicd-system|为cicd流水线提供镜像打包、自动部署支持|
|mysql|为||
||||
||||
||||
||||
||||